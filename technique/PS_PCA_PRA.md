# Plan de Sauvegarde, Continuit√© et Reprise d'Activit√© (PS/PCA/PRA)

**Version :** 1.0  
**Date :** 27/02/2026  
**Auteur(s) :** Pierre MICHEL

[![Type: Documentation Technique](https://img.shields.io/badge/Type-Documentation%20Technique-blue?style=for-the-badge)]() [![Statut: Draft](https://img.shields.io/badge/Statut-Draft-yellow?style=for-the-badge)]() [![Priorit√©: Critique](https://img.shields.io/badge/Priorit√©-Critique-red?style=for-the-badge)]()

## Liens rapides

- [Dossier Projet](../projet/Dossier_Projet.md)
- [CdCF - Cahier des Charges Fonctionnel](../projet/01_CdCF.md)
- [CdCT - Cahier des Charges Technique](../projet/02_CdCT.md)
- [Architecture Technique](./Architecture.md)
- [S√©curit√©](./S√©curit√©.md)
- [DevOps](./DevOps.md)

**Tags :** `#sauvegarde` `#backup` `#pca` `#pra` `#continuit√©-activit√©` `#disaster-recovery` `#r√©silience` `#haute-disponibilit√©`

## Table des mati√®res

1. [Vue d'ensemble et objectifs](#1-vue-densemble-et-objectifs)
2. [Plan de Sauvegarde (PS)](#2-plan-de-sauvegarde-ps)
3. [Plan de Continuit√© d'Activit√© (PCA)](#3-plan-de-continuit√©-dactivit√©-pca)
4. [Plan de Reprise d'Activit√© (PRA)](#4-plan-de-reprise-dactivit√©-pra)
5. [Organisation et gouvernance](#5-organisation-et-gouvernance)
6. [Am√©lioration continue](#6-am√©lioration-continue)

## 1. Vue d'ensemble et objectifs

### 1.1 Contexte et enjeux

La plateforme Taskforce est construite comme un service SaaS multi-tenant destin√© √† g√©rer les projets et t√¢ches de multiples organisations clientes. Dans ce contexte, la disponibilit√© continue et la protection des donn√©es ne sont pas optionnelles : elles conditionnent directement la confiance des clients et la viabilit√© commerciale du service. Ce document √©tablit la strat√©gie globale pour garantir cette r√©silience √† travers trois piliers compl√©mentaires : la sauvegarde des donn√©es, la continuit√© de service en cas d'incident mineur, et la capacit√© de reprise compl√®te apr√®s un sinistre majeur.

### 1.2 Objectifs strat√©giques et indicateurs cl√©s

La strat√©gie de r√©silience repose sur deux objectifs mesurables fondamentaux : le Recovery Time Objective (RTO) qui fixe le d√©lai maximal acceptable pour restaurer un service apr√®s incident, et le Recovery Point Objective (RPO) qui d√©finit la perte de donn√©es maximale tol√©rable. Pour Taskforce, le RTO cible est fix√© √† 4 heures maximum, ce qui signifie qu'en cas de panne compl√®te, le service doit √™tre restaur√© dans ce d√©lai. Le RPO est quant √† lui √©tabli √† 15 minutes, limitant la perte potentielle de donn√©es √† ce court intervalle.

Ces objectifs ambitieux se traduisent concr√®tement par un engagement de disponibilit√© annuelle de 99,9%, soit environ 8 heures d'interruption maximale par an. Pour atteindre ces cibles, l'infrastructure est con√ßue avec plusieurs niveaux de redondance et des m√©canismes automatiques de basculement qui permettent de maintenir le service m√™me en cas de d√©faillance partielle.

| Indicateur | Objectif | Seuil critique |
|------------|----------|----------------|
| **RTO** (temps de restauration) | < 4 heures | 8 heures max |
| **RPO** (perte de donn√©es) | < 15 minutes | 1 heure max |
| **Disponibilit√© annuelle** | 99,9% (8h downtime/an) | 99,5% minimum |
| **Taux de r√©ussite backups** | 99,95% | 99% minimum |

---

## 2. Plan de Sauvegarde (PS)

### 2.1 Strat√©gie globale

La strat√©gie de sauvegarde suit le principe √©prouv√© de la r√®gle 3-2-1 : maintenir trois copies des donn√©es (l'originale en production plus deux sauvegardes), sur deux types de supports diff√©rents (disque et cloud), avec au moins une copie stock√©e hors site dans une r√©gion g√©ographique distincte. Cette approche garantit qu'une d√©faillance unique, qu'elle soit mat√©rielle ou g√©ographique, ne peut jamais entra√Æner de perte de donn√©es.

Le mod√®le de sauvegarde adopt√© privil√©gie la continuit√© plut√¥t que les snapshots isol√©s. PostgreSQL utilise un syst√®me de Write-Ahead Logging (WAL) qui capture chaque modification de la base de donn√©es en temps quasi-r√©el, permettant de restaurer l'√©tat exact de la base √† n'importe quel moment avec une pr√©cision de quelques secondes. Cette approche incr√©mentale continue est compl√©t√©e par des snapshots complets r√©guliers qui servent de points de r√©f√©rence pour acc√©l√©rer les restaurations.

### 2.2 Donn√©es sauvegard√©es et fr√©quence

Les donn√©es critiques de la plateforme incluent la base PostgreSQL compl√®te avec son sch√©ma, les fichiers upload√©s par les utilisateurs (avatars, documents, exports), les configurations syst√®me (variables d'environnement, secrets Kubernetes), et les logs applicatifs n√©cessaires √† l'audit. Chaque cat√©gorie suit une politique de sauvegarde adapt√©e √† sa criticit√© et son volume.

La base de donn√©es b√©n√©ficie d'une protection maximale avec un archivage WAL continu (toutes les 5 minutes ou d√®s que 16 MB de donn√©es sont modifi√©es) et des snapshots complets toutes les 6 heures. Les fichiers utilisateurs sont directement stock√©s sur S3 avec le versioning activ√©, cr√©ant automatiquement une nouvelle version √† chaque modification. Les configurations sont sauvegard√©es √† chaque changement, et les logs sont agr√©g√©s en continu avec une r√©tention de 90 jours pour respecter les besoins d'audit s√©curit√©.

| Type de donn√©es | M√©thode | Fr√©quence | R√©tention |
|-----------------|---------|-----------|-----------|
| Base PostgreSQL | WAL continu + Snapshot | Continue + 6h | WAL 7j, Snapshot 30j |
| Fichiers utilisateurs | S3 versioning | Temps r√©el | 90 jours versions |
| Configurations | Backup incr√©mental | √Ä chaque modification | 365 jours |
| Logs applicatifs | Agr√©gation continue | Temps r√©el | 90 jours |

### 2.3 Infrastructure et r√©plication

L'architecture de sauvegarde s'articule autour de deux r√©gions cloud g√©ographiquement s√©par√©es (Paris et Milan). La r√©gion primaire h√©berge la base de donn√©es en production ainsi que les archives WAL locales. Chaque modification est imm√©diatement archiv√©e localement puis r√©pliqu√©e de mani√®re asynchrone vers un bucket S3 dans la r√©gion primaire. Ce bucket S3 est lui-m√™me configur√© avec une r√©plication cross-region automatique vers la r√©gion secondaire, cr√©ant ainsi trois copies g√©ographiquement distribu√©es des donn√©es.

Pour les fichiers utilisateurs stock√©s directement sur S3, la r√©plication cross-region est √©galement active avec un d√©lai typique inf√©rieur √† 15 minutes. Les anciennes versions de fichiers suivent une politique de cycle de vie automatique : apr√®s 30 jours, elles migrent vers le tier de stockage Infrequent Access moins co√ªteux, puis apr√®s 90 jours vers Glacier pour archivage long terme √† co√ªt minimal.

### 2.4 Tests et validation

La fiabilit√© des sauvegardes ne peut √™tre garantie sans tests r√©guliers de restauration. Un programme structur√© v√©rifie hebdomadairement la capacit√© √† restaurer un fichier individuel, mensuellement la restauration compl√®te d'une base de donn√©es en environnement de test, et trimestriellement la proc√©dure de restauration totale incluant tous les services. Ces tests ne sont pas de simples formalit√©s : chaque restauration est chronom√©tr√©e pour mesurer les RTO/RPO effectifs, et les √©carts par rapport aux objectifs d√©clenchent imm√©diatement des actions correctives.

Le test mensuel de base de donn√©es suit un protocole rigoureux : provisionnement d'une instance PostgreSQL isol√©e, restauration du dernier snapshot complet, application des archives WAL jusqu'√† un point pr√©cis dans le temps, v√©rification de l'int√©grit√© des donn√©es via checksums, connexion de l'application de test pour valider les requ√™tes fonctionnelles, et mesure du temps total de restauration. Les r√©sultats sont syst√©matiquement document√©s dans un ticket de suivi avec identification des axes d'am√©lioration √©ventuels.

### 2.5 Monitoring et alerting

La surveillance automatique des sauvegardes couvre plusieurs dimensions critiques. Chaque √©chec de backup d√©clenche une alerte imm√©diate sur le canal Slack #ops, avec escalade automatique si le probl√®me n'est pas r√©solu dans les 15 minutes. La dur√©e des op√©rations de backup est √©galement surveill√©e : un d√©passement de 4 heures indique un probl√®me de performance n√©cessitant investigation. Des variations soudaines de taille (plus de 50% d'√©cart) alertent sur une possible corruption ou anomalie fonctionnelle.

L'espace disque disponible pour les archives WAL fait l'objet d'une surveillance particuli√®re car son √©puisement bloquerait toute nouvelle transaction. Un seuil d'alerte est fix√© √† 80% d'utilisation pour permettre une intervention pr√©ventive. Enfin, l'√¢ge du dernier backup r√©ussi est v√©rifi√© en continu : un d√©lai sup√©rieur √† 25 heures sans backup valide d√©clenche une alerte critique avec notification du CTO.

---

## 3. Plan de Continuit√© d'Activit√© (PCA)

### 3.1 Philosophie et objectifs

Le Plan de Continuit√© d'Activit√© intervient lors d'incidents de gravit√© interm√©diaire qui d√©gradent les performances ou la disponibilit√© du service sans pour autant le rendre totalement inutilisable. L'objectif n'est pas de maintenir une qualit√© de service parfaite pendant ces p√©riodes, ce qui serait irr√©aliste, mais de pr√©server les fonctionnalit√©s essentielles permettant aux utilisateurs de continuer leur travail avec des limitations acceptables. Cette approche pragmatique reconna√Æt qu'un mode d√©grad√© contr√¥l√© vaut mieux qu'une indisponibilit√© compl√®te.

Le PCA d√©finit plusieurs niveaux de fonctionnement selon la s√©v√©rit√© de l'incident. En mode d√©grad√© l√©ger, toutes les fonctionnalit√©s restent accessibles mais avec une latence accrue et une capacit√© r√©duite √† environ 50% de la charge normale. En mode d√©grad√© s√©v√®re, seules les fonctionnalit√©s critiques sont maintenues (authentification, consultation de projets et t√¢ches) tandis que les fonctions secondaires (notifications, exports, chatbot IA) sont temporairement d√©sactiv√©es. Le temps de basculement vers un mode d√©grad√© ne doit pas exc√©der 30 minutes depuis la d√©tection de l'incident.

### 3.2 Classification des services par criticit√©

Tous les services de la plateforme ne se valent pas en termes d'importance business. L'authentification via Keycloak et la consultation des donn√©es projets/t√¢ches sont class√©es comme critiques : leur indisponibilit√© rend le service compl√®tement inutilisable. Ces fonctions b√©n√©ficient de m√©canismes de fallback sophistiqu√©s comme un cache local d'authentification permettant une autonomie de 1 heure et un mode lecture seule de l'API avec extension du cache.

Les services importants comme la cr√©ation/modification de t√¢ches et les notifications ne sont pas strictement indispensables sur de courtes p√©riodes. En cas d'incident, ces op√©rations peuvent √™tre mises en file d'attente asynchrone avec un traitement diff√©r√©, ou m√™me temporairement d√©sactiv√©es si le mode d√©grad√© doit se prolonger. Les services standards (chatbot IA, g√©n√©ration de rapports) et non-critiques (landing page) peuvent √™tre arr√™t√©s sans impact significatif sur l'activit√© principale des utilisateurs.

| Criticit√© | Services | Mode d√©grad√© | Arr√™t acceptable |
|-----------|----------|--------------|------------------|
| **Critique** | Authentification, Consultation projets/t√¢ches | Cache local 1h, Read-only | Non |
| **Important** | √âcriture API, Notifications | Queue diff√©r√©e | Temporaire (< 4h) |
| **Standard** | Chatbot IA, Exports/rapports | D√©sactivation | Oui |
| **Facultatif** | Landing page | Message maintenance | Oui |

### 3.3 Architecture redondante et basculement

La r√©silience du PCA repose sur une infrastructure g√©ographiquement distribu√©e avec redondance √† plusieurs niveaux. Le load balancer (AWS ALB ou Azure Traffic Manager) surveille la sant√© de chaque instance d'application via des health checks r√©guliers. Si une instance r√©pond avec des erreurs ou pr√©sente une latence excessive, elle est automatiquement retir√©e du pool de routage et le trafic est redistribu√© sur les instances saines. Ce m√©canisme de basculement automatique intervient typiquement en moins d'une minute sans intervention humaine.

La base de donn√©es PostgreSQL maintient en permanence un r√©plica en streaming dans une r√©gion secondaire, synchronis√© avec quelques secondes de d√©calage en conditions normales. Ce r√©plica fonctionne en lecture seule, permettant d'absorber une partie des requ√™tes de consultation et de r√©duire la charge sur la base primaire. En cas d'indisponibilit√© de la r√©gion primaire, le r√©plica peut √™tre promu en ma√Ætre avec une fen√™tre d'intervention de 15 √† 30 minutes incluant validation de l'√©tat des donn√©es et reconfiguration des applications.

### 3.4 Communication de crise

La gestion d'un incident ne se limite pas aux aspects techniques : la communication transparente avec les utilisateurs est cruciale pour maintenir la confiance. D√®s qu'un incident est d√©tect√© et confirm√© (moins de 5 minutes apr√®s d√©clenchement), une notification est publi√©e sur la status page publique avec le statut "Investigating". Cette premi√®re communication reconna√Æt le probl√®me et informe les utilisateurs que l'√©quipe technique est mobilis√©e.

Si l'incident n√©cessite l'activation d'un mode d√©grad√©, une mise √† jour d√©taille pr√©cis√©ment les fonctionnalit√©s affect√©es et celles qui restent disponibles, avec une estimation de dur√©e de r√©solution. Les clients Enterprise b√©n√©ficient d'une communication directe par email dans les 15 minutes. Enfin, lorsque le service est restaur√©, un message de r√©solution indique la dur√©e totale d'incident, l'impact constat√©, et l'engagement de publier un post-mortem d√©taill√© dans les 48 heures suivantes. Cette transparence post-incident transforme chaque probl√®me en opportunit√© d'am√©lioration document√©e.

---

## 4. Plan de Reprise d'Activit√© (PRA)

### 4.1 D√©clenchement et sc√©narios

Le Plan de Reprise d'Activit√© repr√©sente le niveau ultime de r√©ponse face √† un sinistre majeur rendant l'infrastructure primaire totalement inutilisable pour une dur√©e ind√©termin√©e. Son activation n'est pas une d√©cision technique de routine mais une d√©cision strat√©gique prise conjointement par le CTO et la Direction G√©n√©rale, car elle implique des co√ªts significatifs et une fen√™tre d'indisponibilit√© incompressible de plusieurs heures.

Les sc√©narios d√©clenchant le PRA incluent la destruction physique d'un datacenter (catastrophe naturelle, incendie majeur), une cyberattaque de type ransomware ayant chiffr√© les syst√®mes primaires et n√©cessitant une reconstruction compl√®te, une corruption massive des donn√©es sans possibilit√© de r√©paration, ou l'indisponibilit√© prolong√©e d'une r√©gion cloud enti√®re chez le fournisseur. Le crit√®re d√©cisionnel principal est simple : si l'indisponibilit√© pr√©visible d√©passe 2 heures sans perspective de r√©solution rapide, ou si la s√©curit√© des donn√©es est compromise de mani√®re irr√©versible, le PRA est activ√©.

### 4.2 Site de secours et synchronisation

Le site de secours est une infrastructure miroir maintenue en permanence dans une r√©gion g√©ographiquement distincte (Milan pour le secondaire, Paris pour le primaire), distantes de plus de 500 kilom√®tres pour limiter les risques de sinistre commun. Cette infrastructure secondaire n'est pas une simple sauvegarde froide mais un environnement actif recevant en continu les r√©plications de donn√©es et maintenant des images applicatives pr√©-d√©ploy√©es, pr√™tes √† √™tre activ√©es.

La base de donn√©es secondaire fonctionne comme un r√©plica en streaming synchronis√© en quasi-temps r√©el avec la base primaire. Le d√©calage typique (lag) est inf√©rieur √† 30 secondes en conditions normales, pouvant atteindre 5 minutes lors de pics de charge. Les fichiers utilisateurs sur S3 b√©n√©ficient de la r√©plication cross-region automatique avec un d√©lai maximal de 15 minutes. Cette synchronisation continue garantit que le site de secours dispose toujours de donn√©es r√©centes, minimisant la perte potentielle lors d'un basculement d'urgence.

### 4.3 Proc√©dure de reprise en quatre phases

La mise en ≈ìuvre du PRA suit un protocole structur√© en quatre phases distinctes avec des objectifs temporels clairs. La phase 1 (0 √† 30 minutes) concerne l'activation et la mobilisation : d√©claration formelle du PRA, convocation de la cellule de crise en war room, √©valuation de la situation et des causes, publication de la communication pr√©alable informant tous les clients de l'incident majeur en cours. Cette phase √©tablit le cadre organisationnel et communicationnel n√©cessaire √† la suite des op√©rations.

La phase 2 (30 minutes √† 1h30) pr√©pare l'environnement de secours : v√©rification de la sant√© de l'infrastructure secondaire, promotion du r√©plica PostgreSQL en mode lecture/√©criture, validation de l'int√©grit√© des donn√©es restaur√©es par comparaison avec les derni√®res m√©triques connues, reconfiguration des services applicatifs avec les nouvelles coordonn√©es de connexion, et activation du cluster Keycloak secondaire avec restauration de sa base de donn√©es d√©di√©e.

La phase 3 (1h30 √† 3h) bascule le trafic vers le site de secours : modification des enregistrements DNS pour pointer vers les nouvelles adresses IP (avec TTL court de 2 minutes pour propagation rapide), ex√©cution d'une suite de tests fonctionnels critiques couvrant l'authentification et les op√©rations CRUD essentielles, validation des performances avec objectif de temps de r√©ponse inf√©rieur √† 500ms en percentile 95, et activation compl√®te du monitoring sur l'environnement de reprise.

La phase 4 (3h √† 4h) ouvre progressivement le service : retrait de la page de maintenance, activation de rate limiting prudent √† 50% de capacit√© pour limiter les risques, surveillance intensive par toute l'√©quipe technique, communication de restauration sur tous les canaux, et mont√©e en charge graduelle vers 100% de capacit√© sur 4 √† 8 heures selon la stabilit√© observ√©e.

### 4.4 Retour √† la normale

Le retour vers l'infrastructure primaire apr√®s r√©solution du sinistre initial n'est pas moins critique que l'activation du PRA. Il n√©cessite un minimum de 24 heures de stabilit√© compl√®te sur le site de secours et la r√©solution confirm√©e de la cause racine de l'incident. La reconstruction du site primaire peut prendre de 2 √† 5 jours selon l'ampleur des d√©g√¢ts, incluant reprovisionnement de l'infrastructure, restauration des configurations depuis les backups, et synchronisation compl√®te des donn√©es depuis le site de reprise.

Le basculement retour s'effectue lors d'une fen√™tre de maintenance planifi√©e et communiqu√©e 24 heures √† l'avance aux clients. Durant cette fen√™tre de moins d'une heure, le site de secours passe en mode lecture seule, une synchronisation finale des donn√©es vers le site primaire est effectu√©e, la base primaire est promue en mode √©criture, la r√©plication est invers√©e (primaire vers secondaire), et le DNS est bascul√© progressivement. L'√©quipe reste mobilis√©e en surveillance intensive pendant 48 heures suivant le retour pour d√©tecter toute anomalie √©ventuelle.

### 4.5 Post-mortem obligatoire

Chaque activation du PRA fait obligatoirement l'objet d'un rapport post-mortem d√©taill√© publi√© dans les 48 heures suivant le retour √† la normale. Ce document constitue un actif strat√©gique d'apprentissage organisationnel analysant la cause racine de l'incident, les facteurs contributifs, le d√©roulement chronologique pr√©cis des √©v√©nements, l'efficacit√© de la r√©ponse apport√©e avec identification des points forts et des difficult√©s rencontr√©es, et surtout la liste exhaustive des mesures correctives √† court et long terme.

Le post-mortem est partag√© en version compl√®te avec l'√©quipe technique et la direction, en version r√©sum√©e avec les clients Enterprise (focus sur l'impact et les garanties de non-r√©currence), et en version communication publique sur le blog et la status page (transparence sur l'incident et engagement d'am√©lioration). Cette approche de transparence radicale transforme chaque crise en opportunit√© de renforcer la confiance des clients par la demonstration d'une culture d'am√©lioration continue.

---

## 5. Organisation et gouvernance

### 5.1 R√¥les et responsabilit√©s

La gestion de la continuit√© d'activit√© repose sur une organisation clairement d√©finie avec des r√¥les et responsabilit√©s sans ambigu√Øt√©. Le CTO assume la responsabilit√© globale de la strat√©gie PCA/PRA, des d√©cisions budg√©taires associ√©es, et des d√©cisions d'activation lors d'incidents majeurs. Le Lead DevOps coordonne les aspects op√©rationnels au quotidien : maintenance des proc√©dures, organisation et supervision des tests r√©guliers, formation de l'√©quipe.

Le Database Administrator (DBA) porte la responsabilit√© technique des sauvegardes et restaurations de bases de donn√©es, de la supervision de la r√©plication, et de l'optimisation des performances. Le RSSI (Responsable S√©curit√© des Syst√®mes d'Information) veille aux aspects s√©curit√© et conformit√© r√©glementaire. Le VP Customer Success g√®re la communication externe vers les clients pendant les incidents. Chaque r√¥le dispose d'un backup d√©sign√© pour garantir une couverture 24/7, l'astreinte technique tournant sur une base hebdomadaire entre les membres de l'√©quipe DevOps.

### 5.2 Comit√© de pilotage et astreintes

Un comit√© de pilotage trimestriel r√©unit CTO, Lead DevOps, DBA, RSSI, VP Customer Success et CFO pour examiner les incidents du trimestre √©coul√©, analyser les r√©sultats des tests PRA, r√©viser les indicateurs RTO/RPO effectifs, ajuster les budgets et investissements n√©cessaires, int√©grer les √©volutions r√©glementaires, et mettre √† jour les proc√©dures. Ce forum strat√©gique assure que la r√©silience reste une priorit√© align√©e avec les objectifs business.

L'organisation des astreintes garantit une capacit√© de r√©action 24/7 avec une rotation hebdomadaire incluant un DevOps on-call primaire, un DevOps backup pour escalade, et un DBA disponible via PagerDuty. Les outils d'astreinte incluent alerting automatique avec escalade, canal Slack d√©di√© #ops-oncall pour coordination, runbook document√© dans Confluence avec toutes les proc√©dures standard, et acc√®s VPN s√©curis√© √† l'infrastructure de production. Une compensation financi√®re (prime d'astreinte) et en temps de r√©cup√©ration est pr√©vue pour reconna√Ætre cette disponibilit√© √©tendue.

---

## 6. Am√©lioration continue

### 6.1 Programme de tests structur√©

La fiabilit√© des plans de continuit√© ne peut √™tre pr√©sum√©e : elle doit √™tre prouv√©e r√©guli√®rement par des tests r√©els. Un programme structur√© d√©finit plusieurs niveaux de tests avec fr√©quences adapt√©es. Les tests hebdomadaires de restauration d'un fichier individuel, r√©alis√©s par le DevOps d'astreinte en 15 minutes, valident les proc√©dures de base. Les tests mensuels de restauration compl√®te de base de donn√©es, impliquant DBA et DevOps sur 2 heures, mesurent les RTO/RPO effectifs et identifient les points d'optimisation.

Les tests trimestriels simulent un basculement complet vers la r√©gion secondaire avec mobilisation de toute l'√©quipe technique sur 4 heures, validant l'infrastructure redondante et la coordination d'√©quipe. Les simulations semestrielles de PRA complet engagent toute l'entreprise (technique, communication, direction) sur 8 heures dans un exercice r√©aliste incluant les aspects communication clients. Enfin, un exercice annuel de type "tabletop" r√©unit direction et √©quipe technique pour un jeu de r√¥le d'incident majeur, testant la prise de d√©cision strat√©gique sans ex√©cution technique.

### 6.2 Indicateurs et culture d'am√©lioration

Le suivi d'indicateurs cl√©s permet de mesurer objectivement la maturit√© de la r√©silience. Le MTBF (Mean Time Between Failures, temps moyen entre pannes) cible plus de 30 jours, le MTTR (Mean Time To Repair, temps moyen de r√©solution) vise moins d'une heure, et les RTO/RPO effectifs mesur√©s lors des tests sont compar√©s aux objectifs. La disponibilit√© mensuelle cible 99,9% et le taux de r√©ussite des backups 99,95%. Ces m√©triques sont suivies sur des dashboards Grafana avec alertes automatiques en cas de d√©gradation.

Chaque incident, quel que soit sa gravit√©, fait l'objet d'une revue post-incident obligatoire dans les 48 heures avec analyse cause racine (m√©thode 5 Why), √©valuation de l'efficacit√© de la r√©ponse, identification d'actions correctives, et mise √† jour de la documentation. Cette culture de l'apprentissage par l'√©chec transforme chaque probl√®me en opportunit√© d'am√©lioration. Un plan d'√©volution pluriannuel structure les investissements : automatisation compl√®te des tests de backup en Q2 2026, impl√©mentation de chaos engineering r√©gulier en Q3, certification ISO 22301 en Q4, et d√©ploiement d'un troisi√®me site de reprise en Asie √† long terme.

---

**Derni√®re mise √† jour** : 27 f√©vrier 2026  
**Valid√© par** : CTO + Direction G√©n√©rale  
**Prochaine revue** : Mai 2026 (trimestriel)

**Niveau de classification** : üî¥ **CONFIDENTIEL - Usage interne uniquement**
